{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dtr.iloc[:,:-1].values\n",
    "y=dtr.iloc[:,-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35294118 0.74371859 0.59016393 ... 0.50074516 0.23441503 0.48333333]\n",
      " [0.05882353 0.42713568 0.54098361 ... 0.39642325 0.11656704 0.16666667]\n",
      " [0.47058824 0.91959799 0.52459016 ... 0.34724292 0.25362938 0.18333333]\n",
      " ...\n",
      " [0.29411765 0.6080402  0.59016393 ... 0.390462   0.07130658 0.15      ]\n",
      " [0.05882353 0.63316583 0.49180328 ... 0.4485842  0.11571307 0.43333333]\n",
      " [0.05882353 0.46733668 0.57377049 ... 0.45305514 0.10119556 0.03333333]]\n",
      "[1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
      " 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
      " 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
      " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
      " 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state = 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.T\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "x_test = X_test.T\n",
    "y_test = y_test.reshape(1, y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=x_train.shape[0]\n",
    "hidden_layer=12\n",
    "output_layer=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) \n",
    "W1 = np.random.uniform(-1,1,size=(hidden_layer, input_layer))\n",
    "b1 = np.zeros((hidden_layer, 1))\n",
    "W2 = np.random.uniform(-1,1,size=(output_layer, hidden_layer))\n",
    "b2 = np.zeros((output_layer, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(W1,x_train,b1,W2,b2):\n",
    "    \n",
    "    Z1 = np.dot(W1, x_train) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_main(A2,y_train):\n",
    "    \n",
    "    m = y_train.shape[1] \n",
    "    cost= np.multiply(np.log(A2), y_train) + np.multiply((1-y_train), np.log(1 - A2))\n",
    "    cost = - np.sum(cost) / m\n",
    "    cost = float(np.squeeze(cost))\n",
    "    return cost\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(W1,W2, cache, x_train, y_train):\n",
    "    m = x_train.shape[1]\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "   \n",
    "    dZ2 = A2-y_train\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, x_train.T) \n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdw1= np.zeros((W1.shape[0], W1.shape[1]))\n",
    "vdw2= np.zeros((W2.shape[0], W2.shape[1]))\n",
    "vdb1= np.zeros((b1.shape[0], b1.shape[1]))\n",
    "vdb2= np.zeros((b2.shape[0], b2.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W1,b1,W2,b2, grads, learning_rate ,vdw1,vdw2,vdb1,vdb2,beta):\n",
    "   \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    vdw1=beta*vdw1+(1-beta)*dW1\n",
    "    vdw2=beta*vdw2+(1-beta)*dW2\n",
    "    vdb1=beta*vdb1+(1-beta)*db1\n",
    "    vdb2=beta*vdb2+(1-beta)*db2\n",
    "    \n",
    "    W1 = W1 - learning_rate * vdw1\n",
    "    b1 = b1 - learning_rate * vdb1\n",
    "    W2 = W2 - learning_rate * vdw2\n",
    "    b2 = b2 - learning_rate * vdb2\n",
    "    \n",
    "    \n",
    "    return W1,W2,b1,b2,vdw1,vdw2,vdb1,vdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(x_train, y_train,num_iterations,W1,W2,b1,b2,vdw1,vdw2,vdb1,vdb2,learning_rate,beta):\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(W1,x_train,b1,W2,b2)\n",
    "        cost =cost_main(A2,y_train)\n",
    "        grads = backward_propagation(W1,W2, cache, x_train, y_train)\n",
    "        W1,W2,b1,b2,vdw1,vdw2,vdb1,vdb2=gradient_descent(W1,b1,W2,b2, grads, learning_rate ,vdw1,vdw2,vdb1,vdb2,beta)\n",
    "        if i % 5 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    return W1,W2,b1,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.518281\n",
      "Cost after iteration 5: 0.518274\n",
      "Cost after iteration 10: 0.518256\n",
      "Cost after iteration 15: 0.518228\n",
      "Cost after iteration 20: 0.518192\n",
      "Cost after iteration 25: 0.518148\n",
      "Cost after iteration 30: 0.518097\n",
      "Cost after iteration 35: 0.518039\n",
      "Cost after iteration 40: 0.517976\n",
      "Cost after iteration 45: 0.517907\n",
      "Cost after iteration 50: 0.517834\n",
      "Cost after iteration 55: 0.517756\n",
      "Cost after iteration 60: 0.517675\n",
      "Cost after iteration 65: 0.517590\n",
      "Cost after iteration 70: 0.517502\n",
      "Cost after iteration 75: 0.517411\n",
      "Cost after iteration 80: 0.517318\n",
      "Cost after iteration 85: 0.517223\n",
      "Cost after iteration 90: 0.517126\n",
      "Cost after iteration 95: 0.517027\n",
      "Cost after iteration 100: 0.516927\n",
      "Cost after iteration 105: 0.516825\n",
      "Cost after iteration 110: 0.516722\n",
      "Cost after iteration 115: 0.516618\n",
      "Cost after iteration 120: 0.516513\n",
      "Cost after iteration 125: 0.516407\n",
      "Cost after iteration 130: 0.516300\n",
      "Cost after iteration 135: 0.516193\n",
      "Cost after iteration 140: 0.516086\n",
      "Cost after iteration 145: 0.515978\n",
      "Cost after iteration 150: 0.515869\n",
      "Cost after iteration 155: 0.515760\n",
      "Cost after iteration 160: 0.515651\n",
      "Cost after iteration 165: 0.515542\n",
      "Cost after iteration 170: 0.515432\n",
      "Cost after iteration 175: 0.515323\n",
      "Cost after iteration 180: 0.515213\n",
      "Cost after iteration 185: 0.515103\n",
      "Cost after iteration 190: 0.514994\n",
      "Cost after iteration 195: 0.514884\n",
      "Cost after iteration 200: 0.514774\n",
      "Cost after iteration 205: 0.514664\n",
      "Cost after iteration 210: 0.514555\n",
      "Cost after iteration 215: 0.514445\n",
      "Cost after iteration 220: 0.514336\n",
      "Cost after iteration 225: 0.514227\n",
      "Cost after iteration 230: 0.514118\n",
      "Cost after iteration 235: 0.514009\n",
      "Cost after iteration 240: 0.513900\n",
      "Cost after iteration 245: 0.513791\n",
      "Cost after iteration 250: 0.513683\n",
      "Cost after iteration 255: 0.513575\n",
      "Cost after iteration 260: 0.513467\n",
      "Cost after iteration 265: 0.513359\n",
      "Cost after iteration 270: 0.513251\n",
      "Cost after iteration 275: 0.513144\n",
      "Cost after iteration 280: 0.513036\n",
      "Cost after iteration 285: 0.512929\n",
      "Cost after iteration 290: 0.512823\n",
      "Cost after iteration 295: 0.512716\n",
      "Cost after iteration 300: 0.512610\n",
      "Cost after iteration 305: 0.512504\n",
      "Cost after iteration 310: 0.512398\n",
      "Cost after iteration 315: 0.512292\n",
      "Cost after iteration 320: 0.512187\n",
      "Cost after iteration 325: 0.512082\n",
      "Cost after iteration 330: 0.511977\n",
      "Cost after iteration 335: 0.511872\n",
      "Cost after iteration 340: 0.511768\n",
      "Cost after iteration 345: 0.511664\n",
      "Cost after iteration 350: 0.511560\n",
      "Cost after iteration 355: 0.511456\n",
      "Cost after iteration 360: 0.511353\n",
      "Cost after iteration 365: 0.511250\n",
      "Cost after iteration 370: 0.511147\n",
      "Cost after iteration 375: 0.511044\n",
      "Cost after iteration 380: 0.510942\n",
      "Cost after iteration 385: 0.510839\n",
      "Cost after iteration 390: 0.510737\n",
      "Cost after iteration 395: 0.510636\n",
      "Cost after iteration 400: 0.510534\n",
      "Cost after iteration 405: 0.510433\n",
      "Cost after iteration 410: 0.510332\n",
      "Cost after iteration 415: 0.510231\n",
      "Cost after iteration 420: 0.510131\n",
      "Cost after iteration 425: 0.510031\n",
      "Cost after iteration 430: 0.509931\n",
      "Cost after iteration 435: 0.509831\n",
      "Cost after iteration 440: 0.509732\n",
      "Cost after iteration 445: 0.509632\n",
      "Cost after iteration 450: 0.509533\n",
      "Cost after iteration 455: 0.509434\n",
      "Cost after iteration 460: 0.509336\n",
      "Cost after iteration 465: 0.509238\n",
      "Cost after iteration 470: 0.509140\n",
      "Cost after iteration 475: 0.509042\n",
      "Cost after iteration 480: 0.508944\n",
      "Cost after iteration 485: 0.508847\n",
      "Cost after iteration 490: 0.508750\n",
      "Cost after iteration 495: 0.508653\n",
      "Cost after iteration 500: 0.508556\n",
      "Cost after iteration 505: 0.508460\n",
      "Cost after iteration 510: 0.508364\n",
      "Cost after iteration 515: 0.508268\n",
      "Cost after iteration 520: 0.508172\n",
      "Cost after iteration 525: 0.508077\n",
      "Cost after iteration 530: 0.507982\n",
      "Cost after iteration 535: 0.507887\n",
      "Cost after iteration 540: 0.507792\n",
      "Cost after iteration 545: 0.507697\n",
      "Cost after iteration 550: 0.507603\n",
      "Cost after iteration 555: 0.507509\n",
      "Cost after iteration 560: 0.507415\n",
      "Cost after iteration 565: 0.507322\n",
      "Cost after iteration 570: 0.507228\n",
      "Cost after iteration 575: 0.507135\n",
      "Cost after iteration 580: 0.507042\n",
      "Cost after iteration 585: 0.506950\n",
      "Cost after iteration 590: 0.506857\n",
      "Cost after iteration 595: 0.506765\n",
      "Cost after iteration 600: 0.506673\n",
      "Cost after iteration 605: 0.506581\n",
      "Cost after iteration 610: 0.506489\n",
      "Cost after iteration 615: 0.506398\n",
      "Cost after iteration 620: 0.506307\n",
      "Cost after iteration 625: 0.506216\n",
      "Cost after iteration 630: 0.506125\n",
      "Cost after iteration 635: 0.506035\n",
      "Cost after iteration 640: 0.505945\n",
      "Cost after iteration 645: 0.505855\n",
      "Cost after iteration 650: 0.505765\n",
      "Cost after iteration 655: 0.505675\n",
      "Cost after iteration 660: 0.505586\n",
      "Cost after iteration 665: 0.505497\n",
      "Cost after iteration 670: 0.505408\n",
      "Cost after iteration 675: 0.505319\n",
      "Cost after iteration 680: 0.505231\n",
      "Cost after iteration 685: 0.505143\n",
      "Cost after iteration 690: 0.505055\n",
      "Cost after iteration 695: 0.504967\n",
      "Cost after iteration 700: 0.504879\n",
      "Cost after iteration 705: 0.504792\n",
      "Cost after iteration 710: 0.504704\n",
      "Cost after iteration 715: 0.504617\n",
      "Cost after iteration 720: 0.504531\n",
      "Cost after iteration 725: 0.504444\n",
      "Cost after iteration 730: 0.504358\n",
      "Cost after iteration 735: 0.504272\n",
      "Cost after iteration 740: 0.504186\n",
      "Cost after iteration 745: 0.504100\n",
      "Cost after iteration 750: 0.504014\n",
      "Cost after iteration 755: 0.503929\n",
      "Cost after iteration 760: 0.503844\n",
      "Cost after iteration 765: 0.503759\n",
      "Cost after iteration 770: 0.503674\n",
      "Cost after iteration 775: 0.503590\n",
      "Cost after iteration 780: 0.503506\n",
      "Cost after iteration 785: 0.503421\n",
      "Cost after iteration 790: 0.503338\n",
      "Cost after iteration 795: 0.503254\n",
      "Cost after iteration 800: 0.503170\n",
      "Cost after iteration 805: 0.503087\n",
      "Cost after iteration 810: 0.503004\n",
      "Cost after iteration 815: 0.502921\n",
      "Cost after iteration 820: 0.502838\n",
      "Cost after iteration 825: 0.502756\n",
      "Cost after iteration 830: 0.502674\n",
      "Cost after iteration 835: 0.502592\n",
      "Cost after iteration 840: 0.502510\n",
      "Cost after iteration 845: 0.502428\n",
      "Cost after iteration 850: 0.502347\n",
      "Cost after iteration 855: 0.502265\n",
      "Cost after iteration 860: 0.502184\n",
      "Cost after iteration 865: 0.502103\n",
      "Cost after iteration 870: 0.502023\n",
      "Cost after iteration 875: 0.501942\n",
      "Cost after iteration 880: 0.501862\n",
      "Cost after iteration 885: 0.501782\n",
      "Cost after iteration 890: 0.501702\n",
      "Cost after iteration 895: 0.501622\n",
      "Cost after iteration 900: 0.501542\n",
      "Cost after iteration 905: 0.501463\n",
      "Cost after iteration 910: 0.501384\n",
      "Cost after iteration 915: 0.501305\n",
      "Cost after iteration 920: 0.501226\n",
      "Cost after iteration 925: 0.501147\n",
      "Cost after iteration 930: 0.501069\n",
      "Cost after iteration 935: 0.500991\n",
      "Cost after iteration 940: 0.500913\n",
      "Cost after iteration 945: 0.500835\n",
      "Cost after iteration 950: 0.500757\n",
      "Cost after iteration 955: 0.500680\n",
      "Cost after iteration 960: 0.500603\n",
      "Cost after iteration 965: 0.500525\n",
      "Cost after iteration 970: 0.500448\n",
      "Cost after iteration 975: 0.500372\n",
      "Cost after iteration 980: 0.500295\n",
      "Cost after iteration 985: 0.500219\n",
      "Cost after iteration 990: 0.500143\n",
      "Cost after iteration 995: 0.500067\n",
      "Cost after iteration 1000: 0.499991\n",
      "Cost after iteration 1005: 0.499915\n",
      "Cost after iteration 1010: 0.499840\n",
      "Cost after iteration 1015: 0.499764\n",
      "Cost after iteration 1020: 0.499689\n",
      "Cost after iteration 1025: 0.499614\n",
      "Cost after iteration 1030: 0.499540\n",
      "Cost after iteration 1035: 0.499465\n",
      "Cost after iteration 1040: 0.499391\n",
      "Cost after iteration 1045: 0.499316\n",
      "Cost after iteration 1050: 0.499242\n",
      "Cost after iteration 1055: 0.499168\n",
      "Cost after iteration 1060: 0.499095\n",
      "Cost after iteration 1065: 0.499021\n",
      "Cost after iteration 1070: 0.498948\n",
      "Cost after iteration 1075: 0.498875\n",
      "Cost after iteration 1080: 0.498802\n",
      "Cost after iteration 1085: 0.498729\n",
      "Cost after iteration 1090: 0.498656\n",
      "Cost after iteration 1095: 0.498584\n",
      "Cost after iteration 1100: 0.498512\n",
      "Cost after iteration 1105: 0.498439\n",
      "Cost after iteration 1110: 0.498367\n",
      "Cost after iteration 1115: 0.498296\n",
      "Cost after iteration 1120: 0.498224\n",
      "Cost after iteration 1125: 0.498153\n",
      "Cost after iteration 1130: 0.498081\n",
      "Cost after iteration 1135: 0.498010\n",
      "Cost after iteration 1140: 0.497939\n",
      "Cost after iteration 1145: 0.497868\n",
      "Cost after iteration 1150: 0.497798\n",
      "Cost after iteration 1155: 0.497727\n",
      "Cost after iteration 1160: 0.497657\n",
      "Cost after iteration 1165: 0.497587\n",
      "Cost after iteration 1170: 0.497517\n",
      "Cost after iteration 1175: 0.497447\n",
      "Cost after iteration 1180: 0.497378\n",
      "Cost after iteration 1185: 0.497308\n",
      "Cost after iteration 1190: 0.497239\n",
      "Cost after iteration 1195: 0.497170\n",
      "Cost after iteration 1200: 0.497101\n",
      "Cost after iteration 1205: 0.497032\n",
      "Cost after iteration 1210: 0.496963\n",
      "Cost after iteration 1215: 0.496895\n",
      "Cost after iteration 1220: 0.496826\n",
      "Cost after iteration 1225: 0.496758\n",
      "Cost after iteration 1230: 0.496690\n",
      "Cost after iteration 1235: 0.496622\n",
      "Cost after iteration 1240: 0.496555\n",
      "Cost after iteration 1245: 0.496487\n",
      "Cost after iteration 1250: 0.496420\n",
      "Cost after iteration 1255: 0.496353\n",
      "Cost after iteration 1260: 0.496286\n",
      "Cost after iteration 1265: 0.496219\n",
      "Cost after iteration 1270: 0.496152\n",
      "Cost after iteration 1275: 0.496085\n",
      "Cost after iteration 1280: 0.496019\n",
      "Cost after iteration 1285: 0.495953\n",
      "Cost after iteration 1290: 0.495886\n",
      "Cost after iteration 1295: 0.495821\n",
      "Cost after iteration 1300: 0.495755\n",
      "Cost after iteration 1305: 0.495689\n",
      "Cost after iteration 1310: 0.495624\n",
      "Cost after iteration 1315: 0.495558\n",
      "Cost after iteration 1320: 0.495493\n",
      "Cost after iteration 1325: 0.495428\n",
      "Cost after iteration 1330: 0.495363\n",
      "Cost after iteration 1335: 0.495298\n",
      "Cost after iteration 1340: 0.495234\n",
      "Cost after iteration 1345: 0.495169\n",
      "Cost after iteration 1350: 0.495105\n",
      "Cost after iteration 1355: 0.495041\n",
      "Cost after iteration 1360: 0.494977\n",
      "Cost after iteration 1365: 0.494913\n",
      "Cost after iteration 1370: 0.494849\n",
      "Cost after iteration 1375: 0.494786\n",
      "Cost after iteration 1380: 0.494722\n",
      "Cost after iteration 1385: 0.494659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 1390: 0.494596\n",
      "Cost after iteration 1395: 0.494533\n",
      "Cost after iteration 1400: 0.494470\n",
      "Cost after iteration 1405: 0.494407\n",
      "Cost after iteration 1410: 0.494345\n",
      "Cost after iteration 1415: 0.494282\n",
      "Cost after iteration 1420: 0.494220\n",
      "Cost after iteration 1425: 0.494158\n",
      "Cost after iteration 1430: 0.494096\n",
      "Cost after iteration 1435: 0.494034\n",
      "Cost after iteration 1440: 0.493973\n",
      "Cost after iteration 1445: 0.493911\n",
      "Cost after iteration 1450: 0.493850\n",
      "Cost after iteration 1455: 0.493788\n",
      "Cost after iteration 1460: 0.493727\n",
      "Cost after iteration 1465: 0.493666\n",
      "Cost after iteration 1470: 0.493605\n",
      "Cost after iteration 1475: 0.493545\n",
      "Cost after iteration 1480: 0.493484\n",
      "Cost after iteration 1485: 0.493424\n",
      "Cost after iteration 1490: 0.493363\n",
      "Cost after iteration 1495: 0.493303\n",
      "Cost after iteration 1500: 0.493243\n",
      "Cost after iteration 1505: 0.493183\n",
      "Cost after iteration 1510: 0.493124\n",
      "Cost after iteration 1515: 0.493064\n",
      "Cost after iteration 1520: 0.493005\n",
      "Cost after iteration 1525: 0.492945\n",
      "Cost after iteration 1530: 0.492886\n",
      "Cost after iteration 1535: 0.492827\n",
      "Cost after iteration 1540: 0.492768\n",
      "Cost after iteration 1545: 0.492709\n",
      "Cost after iteration 1550: 0.492651\n",
      "Cost after iteration 1555: 0.492592\n",
      "Cost after iteration 1560: 0.492534\n",
      "Cost after iteration 1565: 0.492476\n",
      "Cost after iteration 1570: 0.492417\n",
      "Cost after iteration 1575: 0.492359\n",
      "Cost after iteration 1580: 0.492302\n",
      "Cost after iteration 1585: 0.492244\n",
      "Cost after iteration 1590: 0.492186\n",
      "Cost after iteration 1595: 0.492129\n",
      "Cost after iteration 1600: 0.492071\n",
      "Cost after iteration 1605: 0.492014\n",
      "Cost after iteration 1610: 0.491957\n",
      "Cost after iteration 1615: 0.491900\n",
      "Cost after iteration 1620: 0.491843\n",
      "Cost after iteration 1625: 0.491787\n",
      "Cost after iteration 1630: 0.491730\n",
      "Cost after iteration 1635: 0.491674\n",
      "Cost after iteration 1640: 0.491617\n",
      "Cost after iteration 1645: 0.491561\n",
      "Cost after iteration 1650: 0.491505\n",
      "Cost after iteration 1655: 0.491449\n",
      "Cost after iteration 1660: 0.491394\n",
      "Cost after iteration 1665: 0.491338\n",
      "Cost after iteration 1670: 0.491282\n",
      "Cost after iteration 1675: 0.491227\n",
      "Cost after iteration 1680: 0.491172\n",
      "Cost after iteration 1685: 0.491116\n",
      "Cost after iteration 1690: 0.491061\n",
      "Cost after iteration 1695: 0.491006\n",
      "Cost after iteration 1700: 0.490952\n",
      "Cost after iteration 1705: 0.490897\n",
      "Cost after iteration 1710: 0.490842\n",
      "Cost after iteration 1715: 0.490788\n",
      "Cost after iteration 1720: 0.490734\n",
      "Cost after iteration 1725: 0.490679\n",
      "Cost after iteration 1730: 0.490625\n",
      "Cost after iteration 1735: 0.490571\n",
      "Cost after iteration 1740: 0.490518\n",
      "Cost after iteration 1745: 0.490464\n",
      "Cost after iteration 1750: 0.490410\n",
      "Cost after iteration 1755: 0.490357\n",
      "Cost after iteration 1760: 0.490303\n",
      "Cost after iteration 1765: 0.490250\n",
      "Cost after iteration 1770: 0.490197\n",
      "Cost after iteration 1775: 0.490144\n",
      "Cost after iteration 1780: 0.490091\n",
      "Cost after iteration 1785: 0.490038\n",
      "Cost after iteration 1790: 0.489986\n",
      "Cost after iteration 1795: 0.489933\n",
      "Cost after iteration 1800: 0.489881\n",
      "Cost after iteration 1805: 0.489829\n",
      "Cost after iteration 1810: 0.489776\n",
      "Cost after iteration 1815: 0.489724\n",
      "Cost after iteration 1820: 0.489672\n",
      "Cost after iteration 1825: 0.489621\n",
      "Cost after iteration 1830: 0.489569\n",
      "Cost after iteration 1835: 0.489517\n",
      "Cost after iteration 1840: 0.489466\n",
      "Cost after iteration 1845: 0.489414\n",
      "Cost after iteration 1850: 0.489363\n",
      "Cost after iteration 1855: 0.489312\n",
      "Cost after iteration 1860: 0.489261\n",
      "Cost after iteration 1865: 0.489210\n",
      "Cost after iteration 1870: 0.489159\n",
      "Cost after iteration 1875: 0.489108\n",
      "Cost after iteration 1880: 0.489058\n",
      "Cost after iteration 1885: 0.489007\n",
      "Cost after iteration 1890: 0.488957\n",
      "Cost after iteration 1895: 0.488907\n",
      "Cost after iteration 1900: 0.488857\n",
      "Cost after iteration 1905: 0.488807\n",
      "Cost after iteration 1910: 0.488757\n",
      "Cost after iteration 1915: 0.488707\n",
      "Cost after iteration 1920: 0.488657\n",
      "Cost after iteration 1925: 0.488607\n",
      "Cost after iteration 1930: 0.488558\n",
      "Cost after iteration 1935: 0.488509\n",
      "Cost after iteration 1940: 0.488459\n",
      "Cost after iteration 1945: 0.488410\n",
      "Cost after iteration 1950: 0.488361\n",
      "Cost after iteration 1955: 0.488312\n",
      "Cost after iteration 1960: 0.488263\n",
      "Cost after iteration 1965: 0.488215\n",
      "Cost after iteration 1970: 0.488166\n",
      "Cost after iteration 1975: 0.488117\n",
      "Cost after iteration 1980: 0.488069\n",
      "Cost after iteration 1985: 0.488021\n",
      "Cost after iteration 1990: 0.487972\n",
      "Cost after iteration 1995: 0.487924\n",
      "Cost after iteration 2000: 0.487876\n",
      "Cost after iteration 2005: 0.487828\n",
      "Cost after iteration 2010: 0.487781\n",
      "Cost after iteration 2015: 0.487733\n",
      "Cost after iteration 2020: 0.487685\n",
      "Cost after iteration 2025: 0.487638\n",
      "Cost after iteration 2030: 0.487590\n",
      "Cost after iteration 2035: 0.487543\n",
      "Cost after iteration 2040: 0.487496\n",
      "Cost after iteration 2045: 0.487449\n",
      "Cost after iteration 2050: 0.487402\n",
      "Cost after iteration 2055: 0.487355\n",
      "Cost after iteration 2060: 0.487308\n",
      "Cost after iteration 2065: 0.487262\n",
      "Cost after iteration 2070: 0.487215\n",
      "Cost after iteration 2075: 0.487169\n",
      "Cost after iteration 2080: 0.487122\n",
      "Cost after iteration 2085: 0.487076\n",
      "Cost after iteration 2090: 0.487030\n",
      "Cost after iteration 2095: 0.486984\n",
      "Cost after iteration 2100: 0.486938\n",
      "Cost after iteration 2105: 0.486892\n",
      "Cost after iteration 2110: 0.486846\n",
      "Cost after iteration 2115: 0.486800\n",
      "Cost after iteration 2120: 0.486755\n",
      "Cost after iteration 2125: 0.486709\n",
      "Cost after iteration 2130: 0.486664\n",
      "Cost after iteration 2135: 0.486619\n",
      "Cost after iteration 2140: 0.486573\n",
      "Cost after iteration 2145: 0.486528\n",
      "Cost after iteration 2150: 0.486483\n",
      "Cost after iteration 2155: 0.486438\n",
      "Cost after iteration 2160: 0.486394\n",
      "Cost after iteration 2165: 0.486349\n",
      "Cost after iteration 2170: 0.486304\n",
      "Cost after iteration 2175: 0.486260\n",
      "Cost after iteration 2180: 0.486215\n",
      "Cost after iteration 2185: 0.486171\n",
      "Cost after iteration 2190: 0.486127\n",
      "Cost after iteration 2195: 0.486082\n",
      "Cost after iteration 2200: 0.486038\n",
      "Cost after iteration 2205: 0.485994\n",
      "Cost after iteration 2210: 0.485951\n",
      "Cost after iteration 2215: 0.485907\n",
      "Cost after iteration 2220: 0.485863\n",
      "Cost after iteration 2225: 0.485820\n",
      "Cost after iteration 2230: 0.485776\n",
      "Cost after iteration 2235: 0.485733\n",
      "Cost after iteration 2240: 0.485689\n",
      "Cost after iteration 2245: 0.485646\n",
      "Cost after iteration 2250: 0.485603\n",
      "Cost after iteration 2255: 0.485560\n",
      "Cost after iteration 2260: 0.485517\n",
      "Cost after iteration 2265: 0.485474\n",
      "Cost after iteration 2270: 0.485431\n",
      "Cost after iteration 2275: 0.485389\n",
      "Cost after iteration 2280: 0.485346\n",
      "Cost after iteration 2285: 0.485304\n",
      "Cost after iteration 2290: 0.485261\n",
      "Cost after iteration 2295: 0.485219\n",
      "Cost after iteration 2300: 0.485177\n",
      "Cost after iteration 2305: 0.485134\n",
      "Cost after iteration 2310: 0.485092\n",
      "Cost after iteration 2315: 0.485050\n",
      "Cost after iteration 2320: 0.485009\n",
      "Cost after iteration 2325: 0.484967\n",
      "Cost after iteration 2330: 0.484925\n",
      "Cost after iteration 2335: 0.484883\n",
      "Cost after iteration 2340: 0.484842\n",
      "Cost after iteration 2345: 0.484800\n",
      "Cost after iteration 2350: 0.484759\n",
      "Cost after iteration 2355: 0.484718\n",
      "Cost after iteration 2360: 0.484677\n",
      "Cost after iteration 2365: 0.484635\n",
      "Cost after iteration 2370: 0.484594\n",
      "Cost after iteration 2375: 0.484554\n",
      "Cost after iteration 2380: 0.484513\n",
      "Cost after iteration 2385: 0.484472\n",
      "Cost after iteration 2390: 0.484431\n",
      "Cost after iteration 2395: 0.484391\n",
      "Cost after iteration 2400: 0.484350\n",
      "Cost after iteration 2405: 0.484310\n",
      "Cost after iteration 2410: 0.484269\n",
      "Cost after iteration 2415: 0.484229\n",
      "Cost after iteration 2420: 0.484189\n",
      "Cost after iteration 2425: 0.484149\n",
      "Cost after iteration 2430: 0.484109\n",
      "Cost after iteration 2435: 0.484069\n",
      "Cost after iteration 2440: 0.484029\n",
      "Cost after iteration 2445: 0.483989\n",
      "Cost after iteration 2450: 0.483950\n",
      "Cost after iteration 2455: 0.483910\n",
      "Cost after iteration 2460: 0.483870\n",
      "Cost after iteration 2465: 0.483831\n",
      "Cost after iteration 2470: 0.483792\n",
      "Cost after iteration 2475: 0.483752\n",
      "Cost after iteration 2480: 0.483713\n",
      "Cost after iteration 2485: 0.483674\n",
      "Cost after iteration 2490: 0.483635\n",
      "Cost after iteration 2495: 0.483596\n",
      "Cost after iteration 2500: 0.483557\n",
      "Cost after iteration 2505: 0.483518\n",
      "Cost after iteration 2510: 0.483480\n",
      "Cost after iteration 2515: 0.483441\n",
      "Cost after iteration 2520: 0.483403\n",
      "Cost after iteration 2525: 0.483364\n",
      "Cost after iteration 2530: 0.483326\n",
      "Cost after iteration 2535: 0.483287\n",
      "Cost after iteration 2540: 0.483249\n",
      "Cost after iteration 2545: 0.483211\n",
      "Cost after iteration 2550: 0.483173\n",
      "Cost after iteration 2555: 0.483135\n",
      "Cost after iteration 2560: 0.483097\n",
      "Cost after iteration 2565: 0.483059\n",
      "Cost after iteration 2570: 0.483021\n",
      "Cost after iteration 2575: 0.482984\n",
      "Cost after iteration 2580: 0.482946\n",
      "Cost after iteration 2585: 0.482908\n",
      "Cost after iteration 2590: 0.482871\n",
      "Cost after iteration 2595: 0.482833\n",
      "Cost after iteration 2600: 0.482796\n",
      "Cost after iteration 2605: 0.482759\n",
      "Cost after iteration 2610: 0.482722\n",
      "Cost after iteration 2615: 0.482685\n",
      "Cost after iteration 2620: 0.482648\n",
      "Cost after iteration 2625: 0.482611\n",
      "Cost after iteration 2630: 0.482574\n",
      "Cost after iteration 2635: 0.482537\n",
      "Cost after iteration 2640: 0.482500\n",
      "Cost after iteration 2645: 0.482464\n",
      "Cost after iteration 2650: 0.482427\n",
      "Cost after iteration 2655: 0.482391\n",
      "Cost after iteration 2660: 0.482354\n",
      "Cost after iteration 2665: 0.482318\n",
      "Cost after iteration 2670: 0.482281\n",
      "Cost after iteration 2675: 0.482245\n",
      "Cost after iteration 2680: 0.482209\n",
      "Cost after iteration 2685: 0.482173\n",
      "Cost after iteration 2690: 0.482137\n",
      "Cost after iteration 2695: 0.482101\n",
      "Cost after iteration 2700: 0.482065\n",
      "Cost after iteration 2705: 0.482029\n",
      "Cost after iteration 2710: 0.481994\n",
      "Cost after iteration 2715: 0.481958\n",
      "Cost after iteration 2720: 0.481923\n",
      "Cost after iteration 2725: 0.481887\n",
      "Cost after iteration 2730: 0.481852\n",
      "Cost after iteration 2735: 0.481816\n",
      "Cost after iteration 2740: 0.481781\n",
      "Cost after iteration 2745: 0.481746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 2750: 0.481711\n",
      "Cost after iteration 2755: 0.481676\n",
      "Cost after iteration 2760: 0.481641\n",
      "Cost after iteration 2765: 0.481606\n",
      "Cost after iteration 2770: 0.481571\n",
      "Cost after iteration 2775: 0.481536\n",
      "Cost after iteration 2780: 0.481501\n",
      "Cost after iteration 2785: 0.481467\n",
      "Cost after iteration 2790: 0.481432\n",
      "Cost after iteration 2795: 0.481397\n",
      "Cost after iteration 2800: 0.481363\n",
      "Cost after iteration 2805: 0.481329\n",
      "Cost after iteration 2810: 0.481294\n",
      "Cost after iteration 2815: 0.481260\n",
      "Cost after iteration 2820: 0.481226\n",
      "Cost after iteration 2825: 0.481192\n",
      "Cost after iteration 2830: 0.481158\n",
      "Cost after iteration 2835: 0.481124\n",
      "Cost after iteration 2840: 0.481090\n",
      "Cost after iteration 2845: 0.481056\n",
      "Cost after iteration 2850: 0.481022\n",
      "Cost after iteration 2855: 0.480988\n",
      "Cost after iteration 2860: 0.480955\n",
      "Cost after iteration 2865: 0.480921\n",
      "Cost after iteration 2870: 0.480888\n",
      "Cost after iteration 2875: 0.480854\n",
      "Cost after iteration 2880: 0.480821\n",
      "Cost after iteration 2885: 0.480788\n",
      "Cost after iteration 2890: 0.480754\n",
      "Cost after iteration 2895: 0.480721\n",
      "Cost after iteration 2900: 0.480688\n",
      "Cost after iteration 2905: 0.480655\n",
      "Cost after iteration 2910: 0.480622\n",
      "Cost after iteration 2915: 0.480589\n",
      "Cost after iteration 2920: 0.480556\n",
      "Cost after iteration 2925: 0.480523\n",
      "Cost after iteration 2930: 0.480491\n",
      "Cost after iteration 2935: 0.480458\n",
      "Cost after iteration 2940: 0.480425\n",
      "Cost after iteration 2945: 0.480393\n",
      "Cost after iteration 2950: 0.480360\n",
      "Cost after iteration 2955: 0.480328\n",
      "Cost after iteration 2960: 0.480295\n",
      "Cost after iteration 2965: 0.480263\n",
      "Cost after iteration 2970: 0.480231\n",
      "Cost after iteration 2975: 0.480199\n",
      "Cost after iteration 2980: 0.480167\n",
      "Cost after iteration 2985: 0.480135\n",
      "Cost after iteration 2990: 0.480103\n",
      "Cost after iteration 2995: 0.480071\n"
     ]
    }
   ],
   "source": [
    "W1,W2,b1,b2=neural_network_model(x_train, y_train,3000,W1,W2,b1,b2,vdw1,vdw2,vdb1,vdb2,0.01,0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(W1,W2,b1,b2,x_train):\n",
    "    A2, cache = forward_propagation(W1,x_train,b1,W2,b2)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 77%\n",
      "Accuracy Test: 74%\n"
     ]
    }
   ],
   "source": [
    "predictions =prediction(W1,W2,b1,b2,x_train)\n",
    "print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\n",
    "predictions = prediction(W1,W2,b1,b2,x_test)\n",
    "print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
